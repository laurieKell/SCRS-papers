---
title: "WKBSEABASS North"
subtitle: "Sensitivity Analysis for M and beta"
author: 
  L.T. Kell
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Estimating $F_{MNY}$ for Atlantic Mackerel}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

- **Check paths & add #natM at end of M in control file**

Likelihood profiles can be used to identify where there are data conflicts and whether fixed parameters are supported by the data and what drives the assessment and the grid the importance for management 

Once the Base Cases is agreed a sensitivity analysis for a grid of M and beta can be conducted. These parameters are normally fixed in stock assessments, and have a big impact on the estimates of stock status, reference points, and management advice. It is important to see if there is any information in the data to support the choices made as well as the consequences.


## Methods

### Likehihood Profiles 

Once a base case has been selected, using goodness-of-fit diagnostics a sensitivity analysis can be conducted to determine which datasets, likelihood components and fixed parameters have the biggest effect on estimates of historical and current status relative to reference points. If estimates from alternative plausible models, based on the available evidence and knowledge, fall outside the confidence or credibility intervals of the base case, this suggest that advice based on a single base case may not be robust. In which case an ensemble of models could be run or a variety of Operating Models conditioned as part of an Management Strategy Evaluation (MSE) to evaluate the robustness of advice. 

**Figure 1** shows the likehihood profile and **Figure 2** the likehihood profile by component 

For example, there is seldom information in stock assessment datasets to estimate parameters such as $M$ (natural mortality) and $h$ beta, and so these have either to be fixed or provided as priors. Therefore, a grid based on a range of $M$ and $h$ was used to rerun the assessment. The  contributions of the various components, (i.e age composition, length composition, survey indices, recruitment deviations, catch data, and priors), to the Likelihood can then be profiled to analyse how each component’s likelihood changes as a function of $M$ and $h$. Larger likelihood values indicate greater influence of an a component, and if varies widely as a parameter is varied this it indicates strong sensitivity to that parameter. If minima in the profiles are found at different values of the fuxed parameters this identifies influential but conflicting components. Likelihood Weights (lambda values) assigned to a component will determines it’s influence on the total likelihood. The stability of estimates, can also be considered by perturbing initial values. 

### Production Functions 

The production functions, Yield vs. Spawning Stock Biomass (SSB) across different values of natural mortality ($M$) and beta ($s$) are summarised in **Figure 3**. 

The production curves are concave, with a peak at MSY, showing the biomass level that produces MSY, the dashed line that passes through [BMSY, MSY] indicates $F_{MSY}$. If current catches (red point) are above the curve then the biomass isn't being replaced and the stock will decline, alternatively if it is below the curve then production will be greater than removals and so the stock will increase. For a given level of $F$ if there is no process error then then stock will converge to a corresponding point on the production function. For large process error the stock will cycle anti-clockwise round the production function. THe red kline indicates the current level of $F$, and it's intersect with the production function the equilibrium value for the current $F$. The green quadrant shows where the stock is greater than $B_{MSY}$ and $F$< $F+{MSY}$, the red where the stock is less than $B_{MSY}$ and $F$> $F+{MSY}$.  The green lines identify the ICES target region based on 40% of $B_{0}$. $F$ and yield may be greater than the $MSY$ levels if the stock is greater than $B_{MSY}$, as indicated by the orange triangle. If $F$ is greater than the slope at the origin then the stock will collapse.

## Results

### Likelihood profiles 

The likelihood profile as a function of the fixed parameters $M$ (natural mortality) and $h$ beta,is shown in **Figure 1**, and the relative impacts of the different likelihoods components in **Figure 2**.

The dominant components, i.e. those with largest values, are the age and length compositions. The survey component contributes negatively to the likelihood, but its magnitude is much smaller compared to the composition data sets. The catch, equilibrium catch, and recruitment components make smaller contributions to the log-likelihood, while forecast Recruitment, parameter deviations, and parameter softbounds make negligible contributions. For a beta of 0.8 and an M 80% of the base case value the likilihood is improved, this suggests that a global minima might not have been found for the base case.  

The log-likelihood  changes slightly with $M$, primarily driven by the Age and Length Composition. The other minor components show less variation with $M$, reflecting their limited sensitivity to this parameter. Therefore the age and length composition contributions are key data sources for parameter estimation, since the small contributions from other components imply they have less influence on model outcomes.  This highlights the importance to the assessment of  the age and length Composition when interpreting and refining the SS3 models.

### Production functions

For lower $M$ values (e.g., $M=0.5$) $B_0$ is higher, and the ratio of $B_{MSY}/B_0$ decreases as $M$ increases. $F_{MSY}$ as indicated by the slope of the dashed also increases with $M$, although $MSY$ only increases slightly. The ICES taget region delimited by the green lines is found within the green $MSY$ quadrant. The stock is only acheiving the ICES targets if $M$ is high, as there is an inverse relationship between $M$ and $F$ so that $Z$ stays relatively constant. The red line denotes current $F$ and is only below $F_{MSY}$ for low $M$. beta has less of an effect that $M$, the main effect is on $F$ as beta increases $F_{MSY}$ increases, so that higher fishing levels are sustainable. 


- $MSY$ changes with  $M$, highlighting the importance of natural mortality for management decisions.
- Length and age composition data are critical for model fitting and dominate likelihood contributions.
- Recruitment and survey data play smaller roles but may still influence specific aspects of the model.
  

```{r, knitr, eval=TRUE, echo=FALSE, warning=FALSE, cache=FALSE}
library(knitr)

opts_chunk$set(comment   =NA, 
               warning   =FALSE, 
               message   =FALSE, 
               error     =FALSE, 
               echo      =FALSE, 
               cache     =TRUE, 
               cache.path="../cache/North-Mh/",
               fig.path  ="../figs/North-Mh",
               fig.width =8,
               fig.height=6,
               dev       ="png")

iFig=0
iTab=0
```

```{r, eval=FALSE}
# Install CRAN packages
pkgs_cran=c("ggplot2", "ggpubr", "GGally", "grid", "plyr", "dplyr", 
               "reshape", "data.table", "stringr", "foreach", "doParallel")

install.packages(pkgs_cran)

# Install FLR packages from FLR repository
install.packages(c("FLCore", "FLBRP", "ggplotFL","ss3om"), 
                repos="https://flr.r-universe.dev")

# Install ICES TAF package
install.packages("TAF", repos="https://ices-tools-prod.r-universe.dev")

library(remotes)

# Install FLCandy from GitHub
remotes::install_github("laurieKell/FLCandy")

# Install kobe package
remotes::install_github("fishfollower/kobe")

# Install Stock Synthesis packages
remotes::install_github("r4ss/r4ss")
remotes::install_github("jabbamodel/ss3diags", force=TRUE)

r4ss::get_ss3_exe(dir=model_dir, version="v3.30.22.1")
```

```{r, pkgs}
require(ggplot2)
require(ggpubr)
library(GGally)
library(ggpubr)
library(grid) 

library(patchwork)

library(plyr)
library(dplyr)
library(reshape)

library(TAF)
library(data.table)

library(stringr)
library(gam)

library(FLCore)
library(FLBRP)
library(ggplotFL)
library(FLCandy)
library(kobe)

library(heatmaply)
library(ggcorrplot)

library(r4ss)
library(ss3om)
library(ss3diags)     

library(foreach)
library(doParallel)

theme_set(theme_minimal(base_size=12))
```



```{r, path, eval=!TRUE}
#setwd("/home/laurie/Desktop/SCRS-papers/ss-ll/Rmd")
setwd("c:/active/SCRS-papers/ss-ll/Rmd")
```


```{r, scenarios, echo=TRUE}
scen=expand.grid(M   =c(seq(0.5,1,length.out=5),seq(1,1.5,length.out=5)[-1]),
                 beta=c(0.5,1, 1.5))
ctrl="control.ss"

#path="../data/inputs/00_Model_run_files-20250423T155132Z-001"
path="../data/ICCAT_2017_Model_Run_3_SSv330.23.02/00_Model_run_files"
```


## Figures

```{r, run, eval=FALSE}
cpFiles<-function(sourceDir, destDir) {
    # Remove trailing slashes and asterisks
    sourceDir=gsub("[/*]+$", "", sourceDir)
    
    if (.Platform$OS.type == "windows") {
      cmd=sprintf('xcopy "%s" "%s" /S /I /Y', 
                    sourceDir,
                    destDir)} else {
      cmd=sprintf("find '%s' -type f -exec cp {} '%s' \\;",
                    sourceDir,
                    destDir)}
    
    system(cmd)}

ncore=detectCores()-4  
registerDoParallel(ncore)

# Run
mScen=foreach(i=seq(dim(scen)[1]), 
                .packages=c("TAF","r4ss","ss3om","plyr","dplyr","FLCandy")) %dopar% {
                  
    beta=scen[i,"beta"]                   
    M   =scen[i,"M"] 
    
    # Create dirs
    if (!dir.exists(file.path(path,M)))      mkdir(file.path(path,M))
    if (!dir.exists(file.path(path,M,beta))) mkdir(file.path(path,M,beta))

    # Copy files, excluding directories
    cpFiles(file.path(path,"files/*"), file.path(path,M,beta))

    # Modify control file
    parlines=SS_parlines(file.path(path,M,beta,ctrl))

    sln=parlines[grepl("SR_surv_Beta",c(parlines[,"Label"])),"Linenum"]
    SS_changepars(dir=file.path(path,M,beta), ctlfile=ctrl, newctlfile=ctrl,linenums=seq(sln, sln),newvals=beta)

    ##mln=parlines[grepl("NatM",    c(parlines[,"Label"])),"Linenum"]
    ##SS_changepars(dir=file.path(path,M,beta), ctlfile=ctrl, newctlfile=ctrl,linenums=seq(mln, mln),newvals=M)

    # Modify M vector
    cFl  =scan(file.path(path,M,beta,ctrl),what=as.character(),sep="\n")
    mRow=grep("#natm",tolower(cFl))
    for (iRow in mRow){
       mVec =as.numeric(unlist(strsplit(cFl[iRow], "\\s+")))
       mVec =mVec[!is.na(mVec)]*M
       cFl[iRow]=paste(mVec,collapse=" ")}
    cat(cFl,file=file.path(path,M,beta,ctrl),sep="\n")

    # Run SS3
    r4ss::run(file.path(path,M,beta), exe="ss3.exe", show_in_console=FALSE, skipfinished=FALSE)

    # get SS files
    ss=FLCandy:::tryIt(SS_output(file.path(path,M,beta),verbose=FALSE,hidewarn=FALSE,printstat=FALSE))
    save(ss,file=file.path(path,M,beta,"ss.RData"))
  
    # Create and save FLR objects
    fls=FLCandy:::tryIt(buildFLSss330(ss))
    save(fls,file=file.path(path,M,beta,"fls.RData"))
  
    # Save summary data.frames  
    smry=FLCandy:::tryIt(FLCandy:::smrySS(file.path(path,M,beta)))
    save(smry,file=file.path(path,M,beta,"smry.RData"))
    
    TRUE}

stopImplicitCluster()
```

```{r, runR0, eval=FALSE}
ncore=detectCores()-4  
registerDoParallel(ncore)

# Run
mScen=foreach(i=rev(seq(dim(scen)[1])), 
                .packages=c("TAF","r4ss","ss3om","plyr","dplyr","FLCandy")) %dopar% {
                  
    beta=scen[i,"beta"]                   
    M   =scen[i,"M"] 
    
    load(file.path(path,M,beta,"ss.RData"))
    R0=subset(ss$parameters,Label=="SR_LN(R0)")[,"Value"]
    
    for (iR0 in seq(0.8,1.2,0.1)[-3]){
        if (!dir.exists(file.path(path,M,beta,iR0))) mkdir(file.path(path,M,beta,iR0))

        # Copy files, excluding directories
        cpFiles(file.path(path,"files/*"), file.path(path,M,beta,iR0))
        file.copy(file.path(path,M,beta,ctrl), file.path(path,M,beta,iR0,ctrl),overwrite=TRUE)

        # Modify control file
        parlines=SS_parlines(file.path(path,M,beta,iR0,ctrl))

        sln=parlines[grepl("SR_LN\\(R0\\)",c(parlines[,"Label"])),"Linenum"]
        SS_changepars(dir=file.path(path,M,beta,iR0), ctlfile=ctrl, newctlfile=ctrl,
                          linenums=seq(sln, sln),newphs=-4,newvals=R0*iR0)

        # Run SS3
        r4ss::run(file.path(path,M,beta,iR0), exe="ss3.exe", show_in_console=FALSE, skipfinished=FALSE)

        # get SS files
        ss=FLCandy:::tryIt(SS_output(file.path(path,M,beta,iR0),verbose=FALSE,hidewarn=FALSE,printstat=FALSE))
        save(ss,file=file.path(path,M,beta,iR0,"ss.RData"))
  
        # Create and save FLR objects
        fls=FLCandy:::tryIt(buildFLSss330(ss))
        save(fls,file=file.path(path,M,beta,iR0,"fls.RData"))
  
        # Save summary data.frames  
        smry=FLCandy:::tryIt(FLCandy:::smrySS(file.path(path,M,beta,iR0)))
        save(smry,file=file.path(path,M,beta,iR0,"smry.RData"))
        }
    
    TRUE}

stopImplicitCluster()

dirSS="C:/active/scrs-papers/ss-ll/data/ICCAT_2017_Model_Run_3_SSv330.23.02/00_Model_run_files/0.5/0.5"
R0=subset(ss$parameters,Label=="SR_LN(R0)")[,"Value"]

seq(0.8,1,2,0.1)
parlines=SS_parlines(dirSS)
```

```{r, profile-R0, eval=FALSE}
path ="../data/ICCAT_2017_Model_Run_3_SSv330.23.02/00_Model_run_files/0.5/0.5"
ctrl ="control.ss"
files=c(ctrl,"data.ss","ss3.exe","starter.ss","forecast.ss")

ncore=detectCores()-3  
registerDoParallel(ncore)

# Run
mScen=foreach(iR0=seq(0.75,1.25,length.out=17), 
                .packages=c("TAF","r4ss","ss3om","plyr","dplyr","FLCandy")) %dopar% {

    load(file.path(path,"ss.RData"))
    R0=subset(ss$parameters,Label=="SR_LN(R0)")[,"Value"]
    
    if (!dir.exists(file.path(path,iR0))) mkdir(file.path(path,iR0))

    # Copy files, excluding directories
    file.copy(file.path(path,files), file.path(path,iR0))

    # Modify control file
    parlines=SS_parlines(file.path(path,iR0,ctrl))

    sln=parlines[grepl("SR_LN\\(R0\\)",c(parlines[,"Label"])),"Linenum"]
    SS_changepars(dir=file.path(path,iR0), ctlfile=ctrl, newctlfile=ctrl,
                          linenums=seq(sln, sln),newphs=-4,newvals=R0*iR0)

    # Run SS3
    r4ss::run(file.path(path,iR0), exe="ss3.exe", show_in_console=FALSE, skipfinished=FALSE)

    # get SS files
    ss=FLCandy:::tryIt(SS_output(file.path(path,iR0),verbose=FALSE,hidewarn=FALSE,printstat=FALSE))
    save(ss,file=file.path(path,iR0,"ss.RData"))

    TRUE}

stopImplicitCluster()
```


```{r, LL, fig.height=5, fig.width=10}
scens=FLCandy:::unnest(mlply(scen, function(M,beta) {
            load(file.path(path,M,beta,"smry.RData"))
            smry}))
scens=llply(scens, function(x) cbind(scen[x$Scenario,],x))

ggplot(transform(scens$ll,beta=ac(beta)))+
  geom_line(aes(M,ll,col=beta),linewidth=1.5)+
  xlab("M Multiplier")+
  ylab("Likelihood")+
  theme(legend.position="bottom")+
  theme_bw()
```

**Figure `r iFig=iFig+1; iFig`** Likehihood profile. 

```{r, LL2, fig.height=12,fig.width=12}
lls=mdply(scen, function(M,beta) { 
  load(file.path(path,M,beta,"ss.RData"))
  data.frame(component=dimnames(ss$likelihoods_used)[[1]],
             ll       =ss$likelihoods_used[,-2])})
 
unique(lls$component)
ggplot(transform(subset(lls,!(component%in%c("InitEQ_Regime","Crash_Pen","Equil_catch","Parm_devs"))),
                 beta=ac(beta),
                 component=gsub("_"," ",unique(component))))+
  geom_line(aes(M,ll,col=beta),linewidth=1.5)+
  facet_wrap(.~component,ncol=2,scale="free")+  
  xlab("M Multiplier")+
  ylab("Likelihood")+
  theme_bw()+theme(legend.position="bottom",
                   strip.text.y.left=element_text(angle=0))
```

**Figure `r iFig=iFig+1; iFig`** Likehihood profile by component 

```{r, pFuncs, eval=!FALSE, fig.height=8 ,fig.width=12}
dat=mlply(with(scen,file.path(path,M,beta)),
               function(x)
                     tryIt(SS_output(x,verbose=FALSE,hidewarn=FALSE,printstat=FALSE)))
eql=llply(dat, function(x) tryIt(curveSS(x,maxY=2.5))) 
eql=Map(function(x) cbind(x, scen[x$Scenario,]), FLCandy:::unnest(eql[!unlist(llply(eql,is.null))]))

b40=ddply(eql$curve, .(M,beta), with, {dif=abs(ssb-max(ssb)*0.4)
                               data.frame(ssb  =max(ssb)*0.4,
                                          yield=yield[order(dif)][1])})
b40=rbind.fill(b40[,-4],b40,b40[,-3])
b40$ssb[  is.na(b40$ssb)]  =Inf
b40$yield[is.na(b40$yield)]=0

b40  =subset(b40,         M%in%c(seq(0.5,1.5,0.25)))
rfs  =subset(eql$refpts,  M%in%c(seq(0.5,1.5,0.25)))
crv  =subset(eql$curve,   M%in%c(seq(0.5,1.5,0.25)))
trngl=subset(eql$triangle,M%in%c(seq(0.5,1.5,0.25)))
p=ggplot(rfs)+
  facet_grid(beta~M, labeller=labeller(
     M   =function(x) paste("M *", x),
     beta=function(x) paste("beta =", x)),
     scale="free",space="free")+
  geom_polygon(data=trngl,aes(x, y),                    fill="orange",  alpha=0.5)+
  geom_rect(aes(xmin=0,    xmax=bmsy,ymin=msy,ymax=Inf),fill="#D55E00", alpha=0.3)+
  geom_rect(aes(xmin=bmsy, xmax=Inf, ymin=0,  ymax=msy),fill="#009E73", alpha=0.3)+
  geom_rect(aes(xmin=0,    xmax=bmsy,ymin=0,  ymax=msy),fill="#F0E442", alpha=0.3)+
  geom_rect(aes(xmin=bmsy, xmax=Inf, ymin=msy,ymax=Inf),fill="#F0E442", alpha=0.3)+
  scale_x_continuous(labels=scales::comma)+
  scale_y_continuous(labels=scales::comma)+
  # Lines
  geom_line(data=crv, aes(ssb, yield), color="blue")+
  geom_abline(data=rfs, aes(slope=msy/bmsy, intercept=0), col="grey10", linetype=3) +
  # Reference points
  geom_point(aes(x=bmsy, y=msy), shape=21, fill="white", size=3)+
  geom_text( aes(x=bmsy, y=msy,  label="MSY"), hjust=-0.2, vjust=-0.2)+
  geom_path(aes(ssb,yield),col="green",data=b40)+
  #scale_x_continuous(labels=scales::comma,breaks=seq(0, 100000,by=20000),
  #                   expand=expansion(mult=c(0, 0.05))) +
  scale_color_manual(values= c("blue", "black"),
                    labels= c("Start", "End"),
                    name ="Time Period") +
  labs(title="Production Function",
       x    =expression("Total Biomass (t)"),
       y    =expression("Production (t)"))+
  theme_minimal(base_size=16) +
  theme(panel.grid.minor=element_blank(),
      strip.background=element_rect(fill="grey95"),
      strip.text=element_text(face="bold"),
      axis.title=element_text(face="bold"),
      plot.title=element_text(hjust=0.5, face="bold"),
      legend.position="bottom",
      axis.text.x=element_text(angle=45,hjust=1, size =10,margin=margin(t=10)),
      panel.grid.major.x=element_line(color="grey90"), 
      axis.line=element_line(color="black"))+
   coord_cartesian(xlim=c(0,4e3),ylim=c(0,1.5e3), expand=FALSE)
p
```

**Figure `r iFig=iFig+1; iFig`** Production Functions 

```{r, pFuncs-2, eval=!FALSE, fig.height=8 ,fig.width=12}
# Time series
p+
  geom_point(data=subset(subset(eql$tseries,M%in%seq(0.5,1.5,0.25)), year %in% c(min(year), max(year))),
             aes(ssb, yield/2, color=factor(year)))+
  geom_point(data=subset(subset(eql$tseries,M%in%seq(0.5,1.5,0.25)), !(year %in% c(min(year), max(year)))),
             aes(ssb, yield/2), color="grey10",fill="grey90", shape=21, size=1, stroke=0.2)+
  geom_path(data=subset(eql$tseries,M%in%seq(0.5,1.5,0.25)), aes(ssb, yield/2),
  #            arrow=arrow(length=unit(0.2, "cm"), type="closed",angle=30,ends="last"),
              linewidth=0.25,color="grey10")+
  geom_abline(aes(slope=0.5*yield/ssb,intercept=0),
              data=subset(subset(eql$tseries,M%in%seq(0.5,1.5,0.25)), year==max(year)),col="red")+
   coord_cartesian(xlim=c(0,4e3),ylim=c(0,3e3), expand=FALSE)
```


**Figure `r iFig=iFig+1; iFig`** Production Functions 


```{r, R0}
R0=mdply(scen, function(M,beta) {
            load(file.path(path,M,beta,"ss.RData"))
            subset(ss$parameters,Label=="SR_LN(R0)")[,"Value"]})

ggplot(R0)+
  geom_line(aes(M,V1,col=ac(beta)))+
  labs(y=expression(R[0]),
       colour="beta")
```

**Figure `r iFig=iFig+1; iFig`**

```{r}
eql$refpts=merge(eql$refpts,ddply(eql$curve,.(beta,M), with, data.frame(b0=max(ssb))))
pellat=ddply(eql$refpts,.(M,beta), with, pellatParams(k=b0,bmsy=bmsy,fmsy=(msy/bmsy)))
refs=merge(eql$refpts,pellat)

library(knitr)
library(kableExtra)


# Summarise the values of b0, msy, bmsy, and r by M and beta
smry=aggregate(cbind(b0, msy, bmsy, r) ~ M + beta, data=refs, FUN=mean)

# Create a formatted table
kable(smry, caption="Summary of b0, msy, bmsy, and r by M and beta") %>%
  kable_styling(bootstrap_options=c("striped", "hover", "condensed"), full_width=FALSE)

p=ggplot(reshape2::melt(refs[,c(1:5,7)],c("M","beta")))+
         geom_line(aes(M,value,col=ac(beta)))+
         facet_wrap(~variable,scale="free")
```

```{r}


```

```{r, retro, eval=FALSE}
source("C:/active/FLCandy/R/ss-MVLN.R")

runRetro<-function(i,path,oldsubdir,newsubdir="retros",seed=1224){
  
  retro(dir=path,oldsubdir=oldsubdir,newsubdir=newsubdir,years=-i)
  
  m_ply(c("retro0",paste("retro",i,sep="-")), function(x) {
          ss=FLCandy:::tryIt(SS_output(file.path(path,"retro",x),verbose=FALSE,hidewarn=FALSE,printstat=FALSE))
          save(ss,file=file.path(path,"retro",x,"ss.RData"))})
  
  mvls=mdply(c("retro0",paste("retro",i,sep="-")), function(x) {
           load(file.path(path,"retro",x,"ss.RData"))
           set.seed(seed) 
           tryIt(model.frame(mvlnFLQ(ssmvln(ss$CoVar,ss$derived_quants,mc=500)),drop=T))})
  
  m_ply(c("retro0",paste("retro",i,sep="-")), function(x) {
        load(file.path(path,"retro",x,"ss.RData"))
        fls=FLCandy:::tryIt(buildFLSss330(ss))
        save(fls,file=file.path(path,"retro",x,"fls.RData"))})
      
  FLQuants(dlply(mvls,.(X1), with, as.FLQuant(data.frame(year=year,iter=iter,data=stock))))}

path="P:/rfmo/ices/wkbseabass/ss3/20250129/north"

runRetro(i,path,oldsubdir="base",seed=1224){
    
ggplot(window(flqs,start=2010),aes(x=date,y=data,      
                       colour=qname, fill=qname))+
  geom_flquantiles(aes(colour=qname, fill=qname), probs=c(.25,.5,.75),alpha=0.5)+
  geom_flquantiles(aes(colour=qname, fill=qname), probs=c(.1,.5,.9),  alpha=0.2)+
  geom_hline(aes(yintercept=0.4),col="red")+
  labs(colour="Retro")+
  xlab("Year")+ylab(expression(SSB/B[0]))+
  theme(legend.position="bottom")

```


To compare the profile likelihood curve and the quadratic (Hessian) approximation need to ensure both curves are plotted on the same scale. Specifically, the change in negative log-likelihood ($$\Delta \text{NLL}$$) relative to the minimum value. This is standard practice for likelihood profile diagnostics.

- **Profile likelihood**: plotted as the difference between the negative log-likelihood at each fixed value and the minimum negative log-likelihood (i.e., $$\Delta \text{NLL}=\text{NLL}_i - \text{NLL}_\text{min}$$).
- **Quadratic approximation**: Centered at the MLE and use the standard error from the Hessian.


For each model run (at each fixed $$R_0$$), calculate:
$$
\Delta \text{NLL}_i=\text{NLL}_i - \min(\text{NLL})
$$
where $$\text{NLL}_i$$ is the negative log-likelihood for the $$i$$th profile point.

For a sequence of $$R_0$$ values ($$x$$), use:
$$
\Delta \text{NLL}_\text{quad}(x)=\frac{(x - R_{0,\text{MLE}})^2}{2 \cdot SE^2}
$$
where $$R_{0,\text{MLE}}$$ is the MLE and $$SE$$ is the standard error from the Hessian.


## **Summary Table**

| Step                | Profile Likelihood                          | Quadratic Approximation                    |
|---------------------|---------------------------------------------|--------------------------------------------|
| Y-axis              | $$\Delta \text{NLL}=\text{NLL}_i - \min(\text{NLL})$$ | $$\Delta \text{NLL}=\frac{(R_0 - R_{0,\text{MLE}})^2}{2 SE^2}$$ |
| Centered at         | Minimum NLL (MLE)                           | MLE                                        |
| Interpretation      | Empirical likelihood shape                  | Normal approximation from Hessian          |

```{r}
path="C:/active/scrs-papers/ss-ll/data/ICCAT_2017_Model_Run_3_SSv330.23.02/00_Model_run_files/1/1"
load(file.path(path,"ss.RData"))

R0se=subset(ss$parameters,Label=="SR_LN(R0)")$Parm_StDev
R0  =mdply(file.path(path,seq(0.75,1.25,length.out=17),"ss.RData"),function(x){
            load(x)
            cbind(hat=subset(ss$parameters,Label=="SR_LN(R0)")$Value,
                  ll =ss$likelihoods_used[1,"values"])})
  
## ll contains the NLLs for each profile point, and hat the corresponding R0 values
R0 =transform(R0,deltaNLL=ll-min(ll,na.rm=TRUE))

# Quadratic curve
R0Seq=seq(min(R0$hat), max(R0$hat), length.out=101)
Curve=(R0Seq-R0$hat[which.min(R0$ll)])^2/(2*R0se^2)

ggplot() +
  geom_line(data=data.frame(R0=R0Seq, deltaNLL=Curve),
            aes(x=R0, y=deltaNLL),  color="red", linewidth=1) +
  geom_point(data=R0,
             aes(x=hat, y=deltaNLL), color="blue", size=2) +
  geom_vline(xintercept=R0$hat[which.min(R0$ll)], linetype="dashed") +
  ylab("Change in negative log-likelihood") +
  xlab("SR_LN(R0)")
```

**Figure `r iFig=iFig+1; iFig`** Change in negative log-likelihood from the minimum to ensure the profile and quadratic curves are directly comparable, X-axis: $$R_0$$, Y-axis: $$\Delta \text{NLL}$$ (change in negative log-likelihood from the minimum).

There is a large discrepancy between the profile likelihood points and the Hessian approximation, particularly at the left side of the parameter space (whetre SR_LN(R0) ≈ 4.5). The profile likelihood point is much higher than what the quadratic approximation, indicating strong asymmetry in the likelihood surface.

The Hessian approximation assumes a symmetric, quadratic likelihood surface (corresponding to normally distributed parameter uncertainty). The discrepancy indicates this assumption is violated for SR_LN(R0). Therefore, the standard errors from the Hessian matrix will underestimate the uncertainty of lower values. 

Asymmetry suggests strong non-linearity in how the model responds to changes in R0, which could indicate, model structural issues, R0 is correlated with other parameters, or over-parameterization

The true uncertainty in stock size is likely to be larger than what the Hessian-based confidence intervals would suggest, particularly toward lower stock sizes. While R0 appears identifiable (the likelihood does increase substantially away from the MLE), the asymmetry suggests potential practical non-identifiability issues.

```{r}
p1=ggplot(ss$Kobe)+geom_line(aes(Yr,B.Bmsy))+xlab("Year")+ylab(expression(SSB/B[MSY]))
p2=ggplot(ss$Kobe)+geom_line(aes(Yr,F.Fmsy))+xlab("Year")+ylab(expression(F/F[MSY]))

p1/p2
```

```{r}
path="C:/active/scrs-papers/ss-ll/data/ICCAT_2017_Model_Run_3_SSv330.23.02/00_Model_run_files/1/1"
load(file.path(path, "ss.RData"))

# Extract standard error for B/BMSY
BratioSE=subset(ss$derived_quants, Label=="Bratio_2016")$StdDev
BratioEst=subset(ss$derived_quants, Label=="Bratio_2016")$Value

# 2. Get profile results across different model runs with fixed R0
profile=mdply(file.path(path, seq(0.75, 1.25, length.out=17), "ss.RData"), function(x) {
  load(x)
  # Get B/BMSY value and total likelihood
  cbind(
    R0    =subset(ss$parameters, Label=="SR_LN(R0)")$Value,
    Bratio=subset(ss$derived_quants, Label=="Bratio_2016")$Value,
    ll    =ss$likelihoods_used[1, "values"]
  )
})

# Calculate delta NLL
profile=transform(profile, deltaNLL=ll-min(ll,na.rm=TRUE))

# 3. Create quadratic curve based on Hessian SE for comparison
Bseq=seq(min(profile$Bratio,na.rm=TRUE), max(profile$Bratio,na.rm=TRUE), length.out=101)
quadCurve=(Bseq-BratioEst)^2/(2*BratioSE^2)

# 4. Plot the comparison
ggplot() +
  geom_line(data=data.frame(Bratio=Bseq, deltaNLL=quadCurve),
            aes(x=Bratio, y=deltaNLL), color="red", linewidth=1) +
  geom_point(data=profile,
             aes(x=Bratio, y=deltaNLL), color="blue", size=2) +
  geom_vline(xintercept=BratioEst, linetype="dashed") +
  labs(title="Comparison of Uncertainty Methods for B/BMSY",
       y="Change in negative log-likelihood", 
       x="B/BMSY") +
  theme_minimal()
```

```{r}
path="C:/active/scrs-papers/ss-ll/data/ICCAT_2017_Model_Run_3_SSv330.23.02/00_Model_run_files/1/1"
load(file.path(path, "ss.RData"))

# Extract standard error for B/BMSY
FSE=subset(ss$derived_quants,  Label=="F_2016")$StdDev
Fhat=subset(ss$derived_quants, Label=="F_2016")$Value

# 2. Get profile results across different model runs with fixed R0
profile=mdply(file.path(path, seq(0.75, 1.25, length.out=17), "ss.RData"), function(x) {
  load(x)
  # Get F/FMSY value and total likelihood
  cbind(
    R0=subset(ss$parameters, Label=="SR_LN(R0)")$Value,
    F =subset(ss$derived_quants, Label=="F_2016")$Value,
    ll=ss$likelihoods_used[1, "values"])})

# Calculate delta NLL
profile=transform(profile, deltaNLL=ll-min(ll,na.rm=TRUE))

# Create quadratic curve based on Hessian SE for comparison
Fseq=seq(min(profile$F,na.rm=TRUE), max(profile$F,na.rm=TRUE), length.out=101)
quadCurve=(Fseq-Fhat)^2/(2*FSE^2)

# Plot the comparison
ggplot() +
  geom_line(data=data.frame(F=Fseq, deltaNLL=quadCurve),
            aes(x=F, y=deltaNLL), color="red", linewidth=1) +
  geom_point(data=profile,
             aes(x=F, y=deltaNLL), color="blue", size=2) +
  geom_vline(xintercept=Fhat, linetype="dashed") +
  labs(title="Comparison of Uncertainty Methods for B/BMSY",
       y="Change in negative log-likelihood", 
       x="F/FMSY")+
  theme_minimal()
```

```{r}
path="C:/active/scrs-papers/ss-ll/data/ICCAT_2017_Model_Run_3_SSv330.23.02/00_Model_run_files/1/1"
  
# Path to your model directory
covar_file=file.path(path, "covar.sso")

# Read enough lines to find where the matrix starts
covar_lines=readLines(covar_file, n = 200)
start_line=grep("active-i", covar_lines)-1
if(length(start_line) == 0) stop("Could not find 'active-i' in covar.sso")

# Read the covariance matrix from the file
covar=read.table(covar_file, header = TRUE, skip = start_line)


# Filter for parameter-parameter covariances
covar_par=subset(covar, Par..i=="Par" & Par..j=="Par")
diag     =subset(covar, Par..i=="Par" & Par..j=="Std")

# Get unique parameter names and matrix size
param_names_i=unique(covar_par$label.i)
param_names_j=unique(covar_par$label.j)
param_names  =c(unique(param_names_i,param_names_j))
npar=length(diag[,"label.i"])

# Build the covariance matrix
covmat=matrix(NA, npar, npar, dimnames=list(diag[,"label.i"], diag[,"label.i"]))
for(i in 1:nrow(covar_par)) {
  # Adjust these column names to match your actual data
  rowname=covar_par$label.i[i]
  colname=covar_par$label.j[i]  # This name may differ in your data
  value  =covar_par$corr[i]  # Use your actual value column
  
  # Only add if names are valid
  if(!is.na(rowname)&rowname%in%param_names_i && !is.na(colname)&colname%in%param_names_j) 
       covmat[rowname, colname]=value
  }

# Symmetrize (in case of rounding asymmetry)
covmat[upper.tri(covmat)]=t(covmat)[upper.tri(covmat)]
diag(covmat)=diag[,"corr"]

# Remove NA rows and columns before inverting
na_rows=apply(covmat, 1, function(x) all(is.na(x)))
covmat=covmat[!na_rows, !na_rows]

# Check if the matrix is valid before proceeding
if(nrow(covmat) > 0 && ncol(covmat) > 0) {
  # Invert to get Hessian and calculate eigenvalues
  hessian=try(solve(covmat), silent=TRUE)
  if(!inherits(hessian, "try-error")) {
    eigenvals=eigen(hessian, symmetric=TRUE)$values
   # print(eigenvals)
  } else {
    print("Matrix inversion failed - check for singularity")
  }
} else {
  print("No valid data found in covariance matrix")
}

#smaller values indicate better conditioned problems.
max(abs(eigenvals))/abs(min(abs(eigenvals)))
```

```{r}
save(covmat,file="../data/covmat.RData")
```


```{r}
scen=expand.grid(M   =c(seq(0.5,1,length.out=5),seq(1,1.5,length.out=5)[-1]),
                 beta=c(0.5,1, 1.5))

#path="../data/inputs/00_Model_run_files-20250423T155132Z-001"
path="../data/ICCAT_2017_Model_Run_3_SSv330.23.02/00_Model_run_files"

 srr=mdply(scen, function(M,beta) {
  load(file.path(path,M,beta,"ss.RData"))
  ss$parameters[c("SR_LN(R0)","SR_surv_zfrac","SR_surv_Beta"),"Value"]})

names(srr)[3:5]=c("r0","z","beta")
 
surv3Parm<-function(S, R0, gamma, beta) {
  
  R=(R0*S)/(1+(S/beta)^gamma)
  
  return(R)}


dat=data.frame(S=seq(0,100)*5,
               R=surv3Parm(seq(0,100)*5, 250, 0.2, 1))
ggplot(dat)+geom_line(aes(S,R))
```

```{r}
load("../data/covmat.RData")

# Identify highly correlated pairs
highCor=which(abs(covmat) > 0.7 & 
                          covmat != 1, arr.ind = TRUE)

# Create a data frame of highly correlated pairs
highPairs=data.frame(
  row = rownames(covmat)[highCor[,1]],
  column = colnames(covmat)[highCor[,2]],
  correlation = covmat[highCor])

# Sort by absolute correlation
highPairs=highPairs[order(-abs(highPairs$correlation)),]


ggcorrplot(covmat)
ggcorrplot(covmat[highPairs[,1],highPairs[,2]])

palette = colorRampPalette(c("green", "white", "red"))(20)
heatmap(x = covmat, col = palette, symm = TRUE)
```

```{r}
Original=c(
    "Early_RecrDev_1985", "Early_RecrDev_1986", "Early_RecrDev_1987",
    "Early_RecrDev_1988", "Early_RecrDev_1989", "Main_RecrDev_1990",
    "Main_RecrDev_1991", "Main_RecrDev_1992", "Main_RecrDev_1994",
    "Main_RecrDev_1995", "Main_RecrDev_1996", "Main_RecrDev_1997",
    "Main_RecrDev_1998", "Main_RecrDev_1999", "Main_RecrDev_2000",
    "Main_RecrDev_2001", "Main_RecrDev_2002", "Main_RecrDev_2003",
    "Main_RecrDev_2004", "Main_RecrDev_2006", "Main_RecrDev_2007",
    "Main_RecrDev_2008", "Main_RecrDev_2010", "Main_RecrDev_2011",
    "Main_RecrDev_2012", "Late_RecrDev_2013", "Late_RecrDev_2014",
    "ForeRecr_2016", "LnQ_base_S1_USA_LL_Log(13)", "LnQ_base_S3_JPN_LL(15)",
    "LnQ_base_S4_EU_FOR_LL(16)", "LnQ_base_S6_CTP_LL(18)",
    "Size_DbN_top_logit_F1_EU_LL(1)", "Size_DbN_peak_F1_EU_LL(1)",
    "Size_DbN_ascend_se_F1_EU_LL(1)", "Size_DbN_top_logit_F2_JPN_LL(2)",
    "Size_DbN_peak_F2_JPN_LL(2)", "Size_DbN_ascend_se_F2_JPN_LL(2)",
    "Size_DbN_end_logit_F2_JPN_LL(2)", "SzSel_Fem_Peak_F2_JPN_LL(2)",
    "SzSel_Fem_Final_F2_JPN_LL(2)", "SzSel_Fem_Scale_F2_JPN_LL(2)",
    "Size_DbN_top_logit_F3_CTP_LL(3)", "Size_DbN_peak_F3_CTP_LL(3)", 
    "Size_DbN_ascend_se_F3_CTP_LL(3)", "Size_DbN_end_logit_F3_CTP_LL(3)",
    "SzSel_Male_Peak_F3_CTP_LL(3)", "SzSel_Male_Final_F3_CTP_LL(3)",
    "SzSel_Male_Scale_F3_CTP_LL(3)", "Size_DbN_top_logit_F4_USA_LL(4)",
    "Size_DbN_ascend_se_F4_USA_LL(4)", "Size_DbN_start_logit_F4_USA_LL(4)",
    "SzSel_Fem_Peak_F4_USA_LL(4)", "SzSel_Fem_Final_F4_USA_LL(4)",
    "SzSel_Fem_Scale_F4_USA_LL(4)", "SzSel_Fem_Ascend_F4_USA_LL(4)",
    "SzSel_Fem_Descend_F4_USA_LL(4)", "Size_DbN_top_logit_F5_VEN_LL(5)",
    "Size_DbN_peak_F5_VEN_LL(5)", "Size_DbN_ascend_se_F5_VEN_LL(5)",
    "Size_DbN_end_logit_F5_VEN_LL(5)", "SzSel_Fem_Peak_F5_VEN_LL(5)",
    "SzSel_Fem_Final_F5_VEN_LL(5)", "SzSel_Fem_Scale_F5_VEN_LL(5)",
    "SzSel_Fem_Ascend_F5_VEN_LL(5)", "SzSel_Fem_Descend_F5_VEN_LL(5)")

Friendly=c(
    "Early Recruitment Deviation 1985", "Early Recruitment Deviation 1986",
    "Early Recruitment Deviation 1987", "Early Recruitment Deviation 1988",
    "Early Recruitment Deviation 1989", "Main Recruitment Deviation 1990",
    "Main Recruitment Deviation 1991", "Main Recruitment Deviation 1992",
    "Main Recruitment Deviation 1994", "Main Recruitment Deviation 1995",
    "Main Recruitment Deviation 1996", "Main Recruitment Deviation 1997", 
    "Main Recruitment Deviation 1998", "Main Recruitment Deviation 1999",
    "Main Recruitment Deviation 2000", "Main Recruitment Deviation 2001",
    "Main Recruitment Deviation 2002", "Main Recruitment Deviation 2003",
    "Main Recruitment Deviation 2004", "Main Recruitment Deviation 2006", 
    "Main Recruitment Deviation 2007", "Main Recruitment Deviation 2008",
    "Main Recruitment Deviation 2010", "Main Recruitment Deviation 2011",
    "Main Recruitment Deviation 2012", "Late Recruitment Deviation 2013",
    "Late Recruitment Deviation 2014", "Forecast Recruitment 2016",
    "Log Catchability USA LL S1", "Log Catchability Japan LL S3",
    "Log Catchability EU Foreign LL S4", "Log Catchability CTP LL S6",
    "Top Logit Size Distribution F1 EU LL", "Peak Size Distribution F1 EU LL",
    "Ascend SE Size Distribution F1 EU LL", 
    "Top Logit Size Distribution F2 JPN LL",
    "Peak Size Distribution F2 JPN LL", 
    "Ascend SE Size Distribution F2 JPN LL",
    "End Logit Size Distribution F2 JPN LL", 
    "Female Peak Selectivity F2 JPN LL",
    "Female Final Selectivity F2 JPN LL", 
    "Female Scale Selectivity F2 JPN LL",
    "Top Logit Size Distribution F3 CTP LL", 
    "Peak Size Distribution F3 CTP LL",
    "Ascend SE Size Distribution F3 CTP LL", 
    "End Logit Size Distribution F3 CTP LL",
    "Male Peak Selectivity F3 CTP LL", 
    "Male Final Selectivity F3 CTP LL",
    "Male Scale Selectivity F3 CTP LL", 
    "Top Logit Size Distribution F4 USA LL",
    "Ascend SE Size Distribution F4 USA LL", 
    "Start Logit Size Distribution F4 USA LL",
    "Female Peak Selectivity F4 USA LL", 
    "Female Final Selectivity F4 USA LL",
    "Female Scale Selectivity F4 USA LL", 
    "Female Ascend Selectivity F4 USA LL",
    "Female Descend Selectivity F4 USA LL", 
    "Top Logit Size Distribution F5 VEN LL",
    "Peak Size Distribution F5 VEN LL", 
    "Ascend SE Size Distribution F5 VEN LL",
    "End Logit Size Distribution F5 VEN LL", 
    "Female Peak Selectivity F5 VEN LL",
    "Female Final Selectivity F5 VEN LL", 
    "Female Scale Selectivity F5 VEN LL",
    "Female Ascend Selectivity F5 VEN LL", 
    "Female Descend Selectivity F5 VEN LL")

Sex=ifelse(grepl("Fem", Original), "Female",
              ifelse(grepl("Male", Original), "Male", NA))

Selectivity=ifelse(grepl("Peak", Original, ignore.case = TRUE), "Peak",
            ifelse(grepl("Descend", Original, ignore.case = TRUE), "Descend",
            ifelse(grepl("Ascend", Original, ignore.case = TRUE), "Ascend",
            ifelse(grepl("Final", Original, ignore.case = TRUE), "Final",
            ifelse(grepl("Scale", Original, ignore.case = TRUE), "Scale",
            ifelse(grepl("Start", Original, ignore.case = TRUE), "Start",
            ifelse(grepl("End", Original, ignore.case = TRUE), NA, NA)))))))

Fleet=ifelse(grepl("F[0-9]+_[A-Z]+_LL", Original),
                regmatches(Original, regexpr("F[0-9]+_[A-Z]+_LL", Original)),
                NA)

# Assuming Original is already defined

# Identify size-related parameters
Size=ifelse(grepl("Size", Original, ignore.case = TRUE) |
                    grepl("DbN", Original, ignore.case = TRUE),
                    "Size Parameter", NA)

# Identify recruitment parameters and their types
Recruitment=ifelse(
  grepl("RecrDev|ForeRecr", Original),"Recruitment Deviation", NA)

Year=as.numeric(sub(".*_(\\d{4}).*", "\\1", Original)) 


# Combine with previous variables (example)
meta=data.frame(
  Original,
  Friendly,
  Sex,
  Selectivity,
  Fleet,
  Recruitment,
  Year,
  Size,
  stringsAsFactors = FALSE)
```


### Next steps

Confidence intervals should be based on the profile likelihood rather than the Hessian approximation. Asymmetry may result from correlations between R0 and other parameters (perhaps steepness or natural mortality)[5].  As this parameter is critical to management advice, consider whether model simplification or alternative parameterisation might improve stability. Check ifor systematic patterns in residuals that might explain the asymmetry in the likelihood profile.
